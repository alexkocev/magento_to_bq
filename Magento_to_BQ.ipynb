{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# -------- IMPORTS --------\n",
    "# -------------------------\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import config\n",
    "# Magento 2 Credentials\n",
    "M2_BASE_URL = config.M2_BASE_URL            \n",
    "M2_ACCESS_TOKEN = config.M2_ACCESS_TOKEN\n",
    "M2_USERNAME = config.M2_USERNAME\n",
    "M2_PASSWORD = config.M2_PASSWORD\n",
    "\n",
    "# BigQuery Credentials and Table Information\n",
    "BQ_PATH_KEY = config.BQ_PATH_KEY\n",
    "BQ_PROJECT_ID = config.BQ_PROJECT_ID\n",
    "BQ_DATASET_ID = config.BQ_DATASET_ID\n",
    "BQ_ORDER_TABLE_ID = config.BQ_ORDER_TABLE_ID\n",
    "BQ_CUSTOMER_TABLE_ID = config.BQ_CUSTOMER_TABLE_ID\n",
    "\n",
    "# Date Range for Data Fetching\n",
    "FROM_DATE = config.FROM_DATE\n",
    "TO_DATE = config.TO_DATE\n",
    "\n",
    "# Reset BQ Tables (True to reset data in BigQuery, False if incremental load)\n",
    "RESET = config.RESET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access token received.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# ---    GET NEW M2 TOKEN ----\n",
    "# ----------------------------\n",
    "\n",
    "# Function to fetch data from Magento (with OTP)\n",
    "def get_magento_token():\n",
    "    # Get OTP code from user\n",
    "    M2_OTP_CODE = input(\"Enter the current 6-digit OTP code from your Google Authenticator app: \")\n",
    "    \n",
    "    # Prepare the payload for 2FA\n",
    "    payload = {\n",
    "        \"username\": M2_USERNAME,\n",
    "        \"password\": M2_PASSWORD,\n",
    "        \"otp\": M2_OTP_CODE\n",
    "    }\n",
    "\n",
    "    # Make the POST request to the 2FA authentication endpoint\n",
    "    response = requests.post(f\"{M2_BASE_URL}/rest/V1/tfa/provider/google/authenticate\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
    "\n",
    "    # Check if the authentication is successful\n",
    "    if response.status_code == 200:\n",
    "        access_token = response.json()  # This will be the token you need for subsequent requests\n",
    "        print(\"Access token received.\")\n",
    "        return access_token\n",
    "    else:\n",
    "        print(\"Error fetching token:\", response.text)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "M2_ACCESS_TOKEN = get_magento_token()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# ----- FETCH ORDER AND ITEM DETAILS --------\n",
    "# -------------------------------------------\n",
    "\n",
    "# Headers for authentication\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {M2_ACCESS_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def fetch_orders(from_date, to_date, page=1):\n",
    "    \"\"\"\n",
    "    Fetches orders created between two dates.\n",
    "    Uses pagination to fetch results.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"{M2_BASE_URL}/rest/V1/orders?\"\n",
    "        f\"searchCriteria[filter_groups][0][filters][0][field]=created_at&\"\n",
    "        f\"searchCriteria[filter_groups][0][filters][0][value]={from_date} 00:00:00&\"\n",
    "        f\"searchCriteria[filter_groups][0][filters][0][condition_type]=from&\"\n",
    "        f\"searchCriteria[filter_groups][1][filters][0][field]=created_at&\"\n",
    "        f\"searchCriteria[filter_groups][1][filters][0][value]={to_date} 23:59:59&\"\n",
    "        f\"searchCriteria[filter_groups][1][filters][0][condition_type]=to&\"\n",
    "        f\"searchCriteria[pageSize]=50&\"\n",
    "        f\"searchCriteria[currentPage]={page}\"\n",
    "    )\n",
    "\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Error fetching orders:\", response.text)\n",
    "        return None\n",
    "\n",
    "def format_order_data(orders_data):\n",
    "    \"\"\"\n",
    "    Formats the retrieved order data into a structured dataframe.\n",
    "    Each row corresponds to a single item in the order.\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "\n",
    "    for order in orders_data.get('items', []):\n",
    "        order_id = order.get('entity_id')\n",
    "        created_at = order.get('created_at')\n",
    "        grand_total = order.get('grand_total')\n",
    "        currency = order.get('order_currency_code')\n",
    "        status = order.get('status')\n",
    "\n",
    "        # Customer details\n",
    "        customer_name = f\"{order.get('customer_firstname', '')} {order.get('customer_lastname', '')}\".strip()\n",
    "        customer_email = order.get('customer_email')\n",
    "        billing_address = order.get('billing_address', {})\n",
    "        city = billing_address.get('city', '')\n",
    "        country = billing_address.get('country_id', '')\n",
    "\n",
    "        # Order details\n",
    "        payment = order.get('payment', {})\n",
    "        payment_method = payment.get('method', 'N/A')\n",
    "\n",
    "        # Extract individual items and create a row for each\n",
    "        items = order.get('items', [])\n",
    "        for item in items:\n",
    "            formatted_data.append({\n",
    "                \"Order_ID\": order_id,\n",
    "                \"Date\": created_at,\n",
    "                \"Order_Total\": f\"{grand_total} {currency}\",\n",
    "                \"Order_Status\": status,\n",
    "                \"Customer_Name\": customer_name,\n",
    "                \"Customer_Email\": customer_email,\n",
    "                \"City\": city,\n",
    "                \"Country\": country,\n",
    "                \"Payment_Method\": payment_method,\n",
    "                \"Item_Name\": item.get('name'),\n",
    "                \"SKU\": item.get('sku'),\n",
    "                \"Quantity\": item.get('qty_ordered'),\n",
    "                \"Price_per_Unit\": f\"{item.get('price')} {currency}\",\n",
    "                \"Total_Item_Price\": f\"{item.get('row_total')} {currency}\",\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(formatted_data)\n",
    "\n",
    "def fetch_all_orders(from_date, to_date):\n",
    "    \"\"\"\n",
    "    Fetches all orders between a given date range in an iterative way.\n",
    "    It checks the last date from the fetched orders and continues until all orders are retrieved.\n",
    "    \"\"\"\n",
    "    all_orders_data = []\n",
    "    current_date = from_date\n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f\"Fetching orders for {current_date} (page {page})...\")\n",
    "        orders_data = fetch_orders(current_date, to_date, page)\n",
    "        \n",
    "        if not orders_data or not orders_data.get('items'):\n",
    "            print(\"No more orders found.\")\n",
    "            break\n",
    "\n",
    "        all_orders_data.append(orders_data)\n",
    "        # Check the last order date from the current batch of orders to adjust the current_date\n",
    "        last_order_date = orders_data['items'][-1]['created_at']\n",
    "        current_date = last_order_date.split('T')[0]  # Date part of the last order date\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Combine all fetched data into a single DataFrame\n",
    "    all_formatted_data = []\n",
    "    for orders_data in all_orders_data:\n",
    "        all_formatted_data.append(format_order_data(orders_data))\n",
    "    \n",
    "    return pd.concat(all_formatted_data, ignore_index=True) if all_formatted_data else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# -------       FETCH CUSTOMER DATA     -----\n",
    "# -------------------------------------------\n",
    "\n",
    "def fetch_customers(from_date, to_date, page=1):\n",
    "    url = (\n",
    "        f\"{M2_BASE_URL}/rest/V1/customers/search?\"\n",
    "        f\"searchCriteria[filter_groups][0][filters][0][field]=updated_at&\"\n",
    "        f\"searchCriteria[filter_groups][0][filters][0][value]={from_date} 00:00:00&\"\n",
    "        f\"searchCriteria[filter_groups][0][filters][0][condition_type]=from&\"\n",
    "        f\"searchCriteria[filter_groups][1][filters][0][field]=updated_at&\"\n",
    "        f\"searchCriteria[filter_groups][1][filters][0][value]={to_date} 23:59:59&\"\n",
    "        f\"searchCriteria[filter_groups][1][filters][0][condition_type]=to&\"\n",
    "        f\"searchCriteria[pageSize]=50&\"\n",
    "        f\"searchCriteria[currentPage]={page}\"\n",
    "    )\n",
    "\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching customers: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def fetch_all_customer_groups():\n",
    "    \"\"\"\n",
    "    Fetch all customer groups at once and return as a dictionary mapping ID to name\n",
    "    \"\"\"\n",
    "    groups_dict = {}\n",
    "    try:\n",
    "        print(\"Fetching all customer groups...\")\n",
    "        url = f\"{M2_BASE_URL}/rest/V1/customerGroups/search?searchCriteria[pageSize]=100\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            groups_data = response.json()\n",
    "            for group in groups_data.get('items', []):\n",
    "                group_id = group.get('id')\n",
    "                group_code = group.get('code')\n",
    "                groups_dict[group_id] = group_code\n",
    "            print(f\"Successfully fetched {len(groups_dict)} customer groups\")\n",
    "            return groups_dict\n",
    "        else:\n",
    "            print(f\"Error fetching customer groups: {response.text}\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Exception fetching customer groups: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def format_customer_data(customers_data, customer_groups):\n",
    "    \"\"\"\n",
    "    Formats the retrieved customer data into a structured dataframe.\n",
    "    Each row corresponds to a single customer with detailed information.\n",
    "    \n",
    "    Args:\n",
    "        customers_data (dict): Raw customer data from Magento API\n",
    "        customer_groups (dict): Dictionary mapping group IDs to group names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Formatted customer data\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "    customer_count = len(customers_data.get('items', []))\n",
    "    print(f\"Beginning to format {customer_count} customers...\")\n",
    "\n",
    "    for i, customer in enumerate(customers_data.get('items', [])):\n",
    "        if i % 50 == 0:  # Log progress every 50 customers\n",
    "            print(f\"Formatting customer {i+1}/{customer_count}...\")\n",
    "            \n",
    "        # Basic customer information\n",
    "        customer_id = customer.get('id')\n",
    "        email = customer.get('email')\n",
    "        firstname = customer.get('firstname')\n",
    "        lastname = customer.get('lastname')\n",
    "        created_at = customer.get('created_at')\n",
    "        updated_at = customer.get('updated_at')\n",
    "        \n",
    "        # Get customer group information\n",
    "        group_id = customer.get('group_id')\n",
    "        group_name = customer_groups.get(group_id, f\"Group {group_id}\")\n",
    "        \n",
    "        # Get additional customer attributes\n",
    "        custom_attributes = {attr.get('attribute_code'): attr.get('value') for attr in customer.get('custom_attributes', [])}\n",
    "        \n",
    "        # Extract subscription status\n",
    "        is_subscribed = customer.get('extension_attributes', {}).get('is_subscribed', False)\n",
    "        \n",
    "        # Get addresses if available\n",
    "        addresses = customer.get('addresses', [])\n",
    "        \n",
    "        # Initialize address variables\n",
    "        default_billing_address = None\n",
    "        default_shipping_address = None\n",
    "        all_addresses = []\n",
    "        \n",
    "        # Process addresses\n",
    "        for address in addresses:\n",
    "            address_data = {\n",
    "                'id': address.get('id'),\n",
    "                'city': address.get('city', ''),\n",
    "                'country_id': address.get('country_id', ''),\n",
    "                'firstname': address.get('firstname', ''),\n",
    "                'lastname': address.get('lastname', ''),\n",
    "                'postcode': address.get('postcode', ''),\n",
    "                'telephone': address.get('telephone', ''),\n",
    "                'street': ' '.join(address.get('street', [])),\n",
    "                'region': address.get('region', {}).get('region', ''),\n",
    "                'default_billing': address.get('default_billing', False),\n",
    "                'default_shipping': address.get('default_shipping', False)\n",
    "            }\n",
    "            \n",
    "            all_addresses.append(address_data)\n",
    "            \n",
    "            if address.get('default_billing', False):\n",
    "                default_billing_address = address_data\n",
    "            \n",
    "            if address.get('default_shipping', False):\n",
    "                default_shipping_address = address_data\n",
    "        \n",
    "        # Extract address details\n",
    "        billing_city = default_billing_address.get('city', '') if default_billing_address else ''\n",
    "        billing_country = default_billing_address.get('country_id', '') if default_billing_address else ''\n",
    "        billing_postcode = default_billing_address.get('postcode', '') if default_billing_address else ''\n",
    "        billing_telephone = default_billing_address.get('telephone', '') if default_billing_address else ''\n",
    "        billing_street = default_billing_address.get('street', '') if default_billing_address else ''\n",
    "        billing_region = default_billing_address.get('region', '') if default_billing_address else ''\n",
    "        \n",
    "        shipping_city = default_shipping_address.get('city', '') if default_shipping_address else ''\n",
    "        shipping_country = default_shipping_address.get('country_id', '') if default_shipping_address else ''\n",
    "        shipping_postcode = default_shipping_address.get('postcode', '') if default_shipping_address else ''\n",
    "        shipping_telephone = default_shipping_address.get('telephone', '') if default_shipping_address else ''\n",
    "        shipping_street = default_shipping_address.get('street', '') if default_shipping_address else ''\n",
    "        shipping_region = default_shipping_address.get('region', '') if default_shipping_address else ''\n",
    "        \n",
    "        # Format customer record\n",
    "        formatted_data.append({\n",
    "            \"Customer_ID\": customer_id,\n",
    "            \"Email\": email,\n",
    "            \"First_Name\": firstname,\n",
    "            \"Last_Name\": lastname,\n",
    "            \"Full_Name\": f\"{firstname} {lastname}\",\n",
    "            \"Created_At\": created_at,\n",
    "            \"Updated_At\": updated_at,\n",
    "            \"Group_ID\": group_id,\n",
    "            \"Group_Name\": group_name,\n",
    "            \"Is_Subscribed\": str(is_subscribed),\n",
    "            \n",
    "            # Address information\n",
    "            \"Billing_Street\": billing_street,\n",
    "            \"Billing_City\": billing_city,\n",
    "            \"Billing_Region\": billing_region,\n",
    "            \"Billing_Postcode\": billing_postcode,\n",
    "            \"Billing_Country\": billing_country,\n",
    "            \"Billing_Telephone\": billing_telephone,\n",
    "            \n",
    "            \"Shipping_Street\": shipping_street,\n",
    "            \"Shipping_City\": shipping_city,\n",
    "            \"Shipping_Region\": shipping_region,\n",
    "            \"Shipping_Postcode\": shipping_postcode,\n",
    "            \"Shipping_Country\": shipping_country,\n",
    "            \"Shipping_Telephone\": shipping_telephone,\n",
    "            \n",
    "            # Custom attributes\n",
    "            \"Gender\": custom_attributes.get('gender', ''),\n",
    "            \"Date_Of_Birth\": custom_attributes.get('dob', ''),\n",
    "            \n",
    "            # Add other important custom attributes\n",
    "            \"VAT_Number\": custom_attributes.get('vat_id', ''),\n",
    "            \"Company\": custom_attributes.get('company', ''),\n",
    "            \"Account_Status\": custom_attributes.get('customer_activation', '1'),  # '1' typically means active\n",
    "            \"Total_Address_Count\": len(addresses),\n",
    "            \"Account_Age_Days\": None,  # This will be calculated later if needed\n",
    "        })\n",
    "\n",
    "    print(f\"Completed formatting {customer_count} customers\")\n",
    "    return pd.DataFrame(formatted_data)\n",
    "\n",
    "def fetch_all_customers(from_date, to_date):\n",
    "    all_customers_data = []\n",
    "    page = 1\n",
    "    total_pages = 1\n",
    "    \n",
    "    while page <= total_pages:\n",
    "        print(f\"Fetching customers for date range {from_date} to {to_date} (page {page})...\")\n",
    "        customers_data = fetch_customers(from_date, to_date, page)\n",
    "        \n",
    "        if not customers_data or not customers_data.get('items'):\n",
    "            print(\"No customers found for the specified date range.\")\n",
    "            break\n",
    "\n",
    "        all_customers_data.append(customers_data)\n",
    "        \n",
    "        # Update total pages based on search criteria\n",
    "        total_count = customers_data.get('total_count', 0)\n",
    "        page_size = 100  # This should match the pageSize in the API call\n",
    "        total_pages = (total_count + page_size - 1) // page_size\n",
    "        \n",
    "        print(f\"Retrieved page {page} of {total_pages} (Total customers: {total_count})\")\n",
    "        page += 1\n",
    "        time.sleep(1)  # To avoid hitting API rate limits\n",
    "\n",
    "    print(\"All customer data fetched, beginning processing...\")\n",
    "    \n",
    "    # Fetch all customer groups once (major performance improvement)\n",
    "    customer_groups = fetch_all_customer_groups()\n",
    "\n",
    "    # Combine all fetched data into a single DataFrame\n",
    "    all_formatted_data = []\n",
    "    for i, customers_data in enumerate(all_customers_data):\n",
    "        print(f\"Processing batch {i+1}/{len(all_customers_data)}...\")\n",
    "        all_formatted_data.append(format_customer_data(customers_data, customer_groups))\n",
    "    \n",
    "    df_customers = pd.concat(all_formatted_data, ignore_index=True) if all_formatted_data else pd.DataFrame()\n",
    "\n",
    "    print(f\"Customer data formatted, processing {len(df_customers)} records...\")\n",
    "\n",
    "    # Calculate account age if created_at exists\n",
    "    if not df_customers.empty and 'Created_At' in df_customers.columns:\n",
    "        try:\n",
    "            print(\"Calculating account age...\")\n",
    "            # Convert string dates to datetime objects\n",
    "            df_customers['Created_At_DT'] = pd.to_datetime(df_customers['Created_At'])\n",
    "            current_time = pd.Timestamp.now()\n",
    "            \n",
    "            # Calculate account age in days\n",
    "            df_customers['Account_Age_Days'] = (current_time - df_customers['Created_At_DT']).dt.days\n",
    "            \n",
    "            # Drop the temporary datetime column\n",
    "            df_customers = df_customers.drop(columns=['Created_At_DT'])\n",
    "            print(\"Account age calculation completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate account age: {str(e)}\")\n",
    "    \n",
    "    return df_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# -------          ETL FUNCTIONS        -----\n",
    "# -------------------------------------------\n",
    "\n",
    "# Function to check if BigQuery table is empty\n",
    "def check_table_exists(table_id):\n",
    "    try:\n",
    "        # Try fetching the table to check if it exists\n",
    "        table = client.get_table(f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\")\n",
    "        print(f\"Table {table_id} exists.\")\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        # Check if the error message indicates the table doesn't exist\n",
    "        if \"Not found\" in str(e) or \"notFound\" in str(e):\n",
    "            print(f\"Table {table_id} does not exist.\")\n",
    "            return None\n",
    "        else:\n",
    "            # Re-raise the exception if it's not a \"Not found\" error\n",
    "            print(f\"Error checking table {table_id}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "def fetch_existing_data_from_bq(table_id):\n",
    "    try:\n",
    "        # Check if the table has a schema by getting table metadata\n",
    "        table_ref = f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\"\n",
    "        table = client.get_table(table_ref)\n",
    "        \n",
    "        # If table has no schema, return an empty DataFrame\n",
    "        if not table.schema:\n",
    "            print(f\"Table {table_id} exists but has no schema. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # If table exists and has a schema, query the data\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}`\n",
    "        \"\"\"\n",
    "        query_job = client.query(query)\n",
    "        df_existing = query_job.to_dataframe()\n",
    "        return df_existing\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If the error is due to no schema, return an empty DataFrame\n",
    "        if \"does not have a schema\" in str(e):\n",
    "            print(f\"Table {table_id} exists but has no schema. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "        # For other errors, raise the exception\n",
    "        else:\n",
    "            print(f\"Error fetching data from table {table_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to create BigQuery table schema from Magento data (with table deletion)\n",
    "def create_table_from_data(table_id, df_new):\n",
    "    # Fetch data from Magento to derive schema (use your existing function)\n",
    "    if df_new.empty:\n",
    "        print(\"No new data from Magento to fetch schema.\")\n",
    "        return None\n",
    "    \n",
    "    # Get schema from the first row of the dataframe (use data column names)\n",
    "    schema = [bigquery.SchemaField(col, \"STRING\") for col in df_new.columns]\n",
    "    \n",
    "    # Check if the table exists, and delete it before recreating\n",
    "    try:\n",
    "        # Check if table exists\n",
    "        client.get_table(f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\")\n",
    "        print(f\"Table {table_id} already exists. Deleting the table.\")\n",
    "        client.delete_table(f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\")  # Delete the table\n",
    "    except bigquery.exceptions.NotFound:\n",
    "        print(f\"Table {table_id} does not exist. Proceeding to create a new one.\")\n",
    "    \n",
    "    # Recreate the table with the new schema\n",
    "    table = bigquery.Table(f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\", schema=schema)\n",
    "    client.create_table(table)  # Create the table with the inferred schema\n",
    "    print(f\"Table {table_id} has been created with schema from Magento data.\")\n",
    "    return df_new\n",
    "\n",
    "\n",
    "\n",
    "def reset_bigquery_table(table_id):\n",
    "    print(f\"Resetting data and schema in BigQuery table {table_id}...\")\n",
    "    \n",
    "    # Step 1: Delete the table (this removes all data and schema)\n",
    "    client.delete_table(f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\", not_found_ok=True)\n",
    "    print(f\"Table {table_id} has been deleted.\")\n",
    "    \n",
    "    # Step 2: Recreate the table with an empty schema\n",
    "    schema = []  # Empty schema\n",
    "    table = bigquery.Table(f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\", schema=schema)\n",
    "    client.create_table(table)  # Recreate the table\n",
    "    print(f\"Table {table_id} has been recreated with a new schema.\")\n",
    "    \n",
    "\n",
    "# Compare and update the data in BQ table\n",
    "def compare_and_update_data(df_new, df_existing, id_column):\n",
    "    # Check if the DataFrames are empty\n",
    "    if df_new.empty:\n",
    "        print(f\"No new data provided. Skipping comparison.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Check if id_column exists in both DataFrames\n",
    "    if id_column not in df_new.columns:\n",
    "        print(f\"Error: '{id_column}' column not found in new data. Available columns: {df_new.columns.tolist()}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    if df_existing.empty:\n",
    "        print(f\"No existing data found. All new data will be treated as new records.\")\n",
    "        # Create a DataFrame with the same structure as df_new but empty\n",
    "        df_empty = pd.DataFrame(columns=df_new.columns)\n",
    "        # Ensure consistent data types for the merge\n",
    "        if id_column in df_empty.columns:\n",
    "            df_empty[id_column] = df_empty[id_column].astype(str)\n",
    "        df_new[id_column] = df_new[id_column].astype(str)\n",
    "        # Perform outer merge to identify new records\n",
    "        df_combined = pd.merge(df_empty, df_new, on=id_column, how=\"outer\", suffixes=(\"_old\", \"_new\"), indicator=True)\n",
    "        return df_combined[df_combined[\"_merge\"] == \"right_only\"], pd.DataFrame()\n",
    "    \n",
    "    if id_column not in df_existing.columns:\n",
    "        print(f\"Error: '{id_column}' column not found in existing data. Available columns: {df_existing.columns.tolist()}\")\n",
    "        print(\"Treating all new data as new records.\")\n",
    "        # Create a new DataFrame with just the id_column\n",
    "        df_empty = pd.DataFrame({id_column: []})\n",
    "        df_empty[id_column] = df_empty[id_column].astype(str)\n",
    "        df_new[id_column] = df_new[id_column].astype(str)\n",
    "        df_combined = pd.merge(df_empty, df_new, on=id_column, how=\"outer\", suffixes=(\"_old\", \"_new\"), indicator=True)\n",
    "        return df_combined[df_combined[\"_merge\"] == \"right_only\"], pd.DataFrame()\n",
    "    \n",
    "    # Print the data types for debugging\n",
    "    print(f\"Data type of {id_column} in new data: {df_new[id_column].dtype}\")\n",
    "    print(f\"Data type of {id_column} in existing data: {df_existing[id_column].dtype}\")\n",
    "    \n",
    "    # Convert both DataFrames' ID columns to the same type (string)\n",
    "    df_new = df_new.copy()\n",
    "    df_existing = df_existing.copy()\n",
    "    \n",
    "    df_new[id_column] = df_new[id_column].astype(str)\n",
    "    df_existing[id_column] = df_existing[id_column].astype(str)\n",
    "    \n",
    "    # Print the data types after conversion\n",
    "    print(f\"After conversion - Data type of {id_column} in new data: {df_new[id_column].dtype}\")\n",
    "    print(f\"After conversion - Data type of {id_column} in existing data: {df_existing[id_column].dtype}\")\n",
    "    \n",
    "    # Now perform the merge with consistent data types\n",
    "    df_combined = pd.merge(df_existing, df_new, on=id_column, how=\"outer\", suffixes=(\"_old\", \"_new\"), indicator=True)\n",
    "    \n",
    "    # Find new and updated records\n",
    "    new_records = df_combined[df_combined[\"_merge\"] == \"right_only\"]\n",
    "    updated_records = df_combined[df_combined[\"_merge\"] == \"both\"]\n",
    "    \n",
    "    # Check for actual differences in content between old and new versions\n",
    "    # Get all columns except the ID column and \"_merge\" indicator\n",
    "    value_columns = [col for col in updated_records.columns \n",
    "                    if not col.endswith('_old') and not col.endswith('_new') \n",
    "                    and col != '_merge' and col != id_column]\n",
    "    \n",
    "    # For each value column, check if old and new values are different\n",
    "    has_changes = False\n",
    "    for col in value_columns:\n",
    "        old_col = f\"{col}_old\"\n",
    "        new_col = f\"{col}_new\"\n",
    "        \n",
    "        if old_col in updated_records.columns and new_col in updated_records.columns:\n",
    "            # Check if any row has different values between old and new\n",
    "            has_diff = (updated_records[old_col] != updated_records[new_col]).any()\n",
    "            if has_diff:\n",
    "                has_changes = True\n",
    "                break\n",
    "    \n",
    "    # If no actual changes found, return empty DataFrame for updated_records\n",
    "    if not has_changes:\n",
    "        updated_records = pd.DataFrame()\n",
    "    else:\n",
    "        # Keep only rows where there are actual differences\n",
    "        updated_records = updated_records[updated_records.filter(like=\"_new\").ne(updated_records.filter(like=\"_old\")).any(axis=1)]\n",
    "    \n",
    "    return new_records, updated_records\n",
    "\n",
    "\n",
    "# Upload new records to BigQuery\n",
    "def upload_to_bq(df_new, table_id):\n",
    "    # Clean up the DataFrame by removing suffix columns and merge indicator\n",
    "    cols_to_drop = [col for col in df_new.columns if col.endswith('_old') or col.endswith('_new') or col == '_merge']\n",
    "    df_new = df_new.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Generate the full table ID\n",
    "    table_full_id = f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}\"\n",
    "    \n",
    "    # Upload to BigQuery\n",
    "    pandas_gbq.to_gbq(df_new, destination_table=table_full_id, project_id=BQ_PROJECT_ID, if_exists='append')\n",
    "    print(f'New records uploaded successfully to table {table_id}!')\n",
    "   \n",
    "   \n",
    "def update_existing_data_in_bq(df_updated, table_id, id_column):\n",
    "    print(f\"Starting to update {len(df_updated)} records in {table_id}...\")\n",
    "\n",
    "    # Convert the DataFrame to a list of dictionaries\n",
    "    rows_to_update = df_updated.to_dict(orient='records')\n",
    "\n",
    "    # Add a counter to track progress\n",
    "    processed = 0\n",
    "    total = len(rows_to_update)\n",
    "    \n",
    "    # Use BigQuery's MERGE query to update the table with modified records\n",
    "    for row in rows_to_update:\n",
    "        # Track columns that have already been processed to avoid duplicates\n",
    "        processed_columns = set()\n",
    "        \n",
    "        # Initialize the lists to hold query clauses\n",
    "        set_clause = []\n",
    "        insert_columns = [id_column]  # Always insert the ID column\n",
    "        insert_values = [f\"'{row[id_column]}'\"]\n",
    "\n",
    "        # Loop over all columns in the row and check for '_new' versions first\n",
    "        for col in row.keys():\n",
    "            if col.endswith('_new'):\n",
    "                base_col = col.replace('_new', '')\n",
    "                \n",
    "                # Skip if we've already processed this column or if it's the ID column\n",
    "                if base_col in processed_columns or base_col == id_column:\n",
    "                    continue\n",
    "                \n",
    "                # Mark this column as processed\n",
    "                processed_columns.add(base_col)\n",
    "                \n",
    "                # Add SET clause to update the column in BigQuery\n",
    "                # Use appropriate formatting and escaping for the value\n",
    "                value = row[col]\n",
    "                if value is not None:\n",
    "                    # Escape single quotes in string values\n",
    "                    if isinstance(value, str):\n",
    "                        value = value.replace(\"'\", \"''\")\n",
    "                    set_clause.append(f\"T.{base_col} = '{value}'\")\n",
    "                \n",
    "        # Increment and log progress every 10 records\n",
    "        processed += 1\n",
    "        if processed % 10 == 0 or processed == total:\n",
    "            print(f\"Processed {processed}/{total} records ({processed/total*100:.1f}%)...\")\n",
    "                    \n",
    "            \n",
    "            \n",
    "        # Prepare the SET clause and ensure it's not empty\n",
    "        if not set_clause:\n",
    "            print(f\"No changes detected for {id_column} {row[id_column]}. Skipping update.\")\n",
    "            continue  # Skip if no changes detected\n",
    "        \n",
    "        # Construct the query with dynamic SET clause\n",
    "        query = f\"\"\"\n",
    "        MERGE `{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{table_id}` AS T\n",
    "        USING (SELECT '{row[id_column]}' AS id) AS S\n",
    "        ON T.{id_column} = S.id\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                {', '.join(set_clause)}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({', '.join(insert_columns)}) \n",
    "            VALUES ({', '.join(insert_values)});\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the query to update BigQuery\n",
    "        try:\n",
    "            client.query(query).result()  # Execute the query and wait for the result\n",
    "            print(f'Updated {id_column} {row[id_column]} in BigQuery table {table_id}.')\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating {id_column} {row[id_column]}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# -------         MAIN FUNCTION         -----\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "def process_data_type(data_type, from_date, to_date, table_id, id_column):\n",
    "    print(f\"Processing {data_type} data...\")\n",
    "    \n",
    "    if RESET == \"True\":\n",
    "        reset_bigquery_table(table_id)\n",
    "\n",
    "\n",
    "    # Step 1: Fetch new data based on data type\n",
    "    if data_type == 'orders':\n",
    "        df_new = fetch_all_orders(from_date, to_date)\n",
    "    elif data_type == 'customers':\n",
    "        df_new = fetch_all_customers(from_date, to_date)\n",
    "    else:\n",
    "        print(f\"Unsupported data type: {data_type}\")\n",
    "        return\n",
    "    \n",
    "    if df_new.empty:\n",
    "        print(f\"No {data_type} data found for the specified date range.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Check if the table exists\n",
    "    table = check_table_exists(table_id)\n",
    "    \n",
    "    # Step 3: Process based on table existence and schema\n",
    "    if table is None:\n",
    "        # Table doesn't exist - create it with data\n",
    "        print(f\"BigQuery table {table_id} does not exist. Creating table schema from data...\")\n",
    "        create_table_from_data(table_id, df_new)\n",
    "        # Upload all data as new\n",
    "        upload_to_bq(df_new, table_id)\n",
    "        print(f\"Created new table {table_id} and uploaded {len(df_new)} records.\")\n",
    "    else:\n",
    "        # Check if the table has a schema\n",
    "        try:\n",
    "            if not table.schema:\n",
    "                # Table exists but has no schema - recreate it with data\n",
    "                print(f\"BigQuery table {table_id} exists but has no schema. Recreating with data...\")\n",
    "                create_table_from_data(table_id, df_new)\n",
    "                # Upload all data as new\n",
    "                upload_to_bq(df_new, table_id)\n",
    "                print(f\"Recreated table {table_id} with schema and uploaded {len(df_new)} records.\")\n",
    "                return\n",
    "        except Exception as e:\n",
    "            # If we can't check schema, we'll continue and handle errors in fetch_existing_data_from_bq\n",
    "            print(f\"Warning: Could not verify schema for table {table_id}: {str(e)}\")\n",
    "        \n",
    "        print(f\"BigQuery table {table_id} exists. Fetching existing data...\")\n",
    "        # Fetch existing data\n",
    "        df_existing = fetch_existing_data_from_bq(table_id)\n",
    "        \n",
    "        # If df_existing is empty but the table exists (no schema or empty table)\n",
    "        if df_existing.empty:\n",
    "            print(f\"Table {table_id} exists but is empty or has no schema. Creating schema from new data...\")\n",
    "            create_table_from_data(table_id, df_new)\n",
    "            # Upload all data as new\n",
    "            upload_to_bq(df_new, table_id)\n",
    "            print(f\"Recreated table {table_id} with schema and uploaded {len(df_new)} records.\")\n",
    "        else:\n",
    "            # Normal flow - compare and update data\n",
    "            print(\"Beginning BigQuery operations...\") \n",
    "            new_records, updated_records = compare_and_update_data(df_new, df_existing, id_column)\n",
    "            print(f\"Data comparison complete. Found {len(new_records)} new and {len(updated_records)} updated records.\")\n",
    "\n",
    "            # Insert new records into BigQuery\n",
    "            if not new_records.empty:\n",
    "                print(f\"Found {len(new_records)} new {data_type} to upload.\")\n",
    "                upload_to_bq(new_records, table_id)\n",
    "            else:\n",
    "                print(f\"No new {data_type} found.\")\n",
    "                \n",
    "            # Update existing records in BigQuery\n",
    "            if not updated_records.empty:\n",
    "                print(f\"Found {len(updated_records)} {data_type} to update.\")\n",
    "                update_existing_data_in_bq(updated_records, table_id, id_column)\n",
    "            else:\n",
    "                print(f\"No {data_type} updates found.\")\n",
    "    \n",
    "    print(f\"Completed processing {data_type} data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing customers data...\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 1)...\n",
      "Retrieved page 1 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 2)...\n",
      "Retrieved page 2 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 3)...\n",
      "Retrieved page 3 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 4)...\n",
      "Retrieved page 4 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 5)...\n",
      "Retrieved page 5 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 6)...\n",
      "Retrieved page 6 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 7)...\n",
      "Retrieved page 7 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 8)...\n",
      "Retrieved page 8 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 9)...\n",
      "Retrieved page 9 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 10)...\n",
      "Retrieved page 10 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 11)...\n",
      "Retrieved page 11 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 12)...\n",
      "Retrieved page 12 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 13)...\n",
      "Retrieved page 13 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 14)...\n",
      "Retrieved page 14 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 15)...\n",
      "Retrieved page 15 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 16)...\n",
      "Retrieved page 16 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 17)...\n",
      "Retrieved page 17 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 18)...\n",
      "Retrieved page 18 of 19 (Total customers: 1877)\n",
      "Fetching customers for date range 2025-01-02 to 2025-01-03 (page 19)...\n",
      "Retrieved page 19 of 19 (Total customers: 1877)\n",
      "All customer data fetched, beginning processing...\n",
      "Fetching all customer groups...\n",
      "Successfully fetched 6 customer groups\n",
      "Processing batch 1/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 2/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 3/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 4/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 5/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 6/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 7/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 8/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 9/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 10/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 11/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 12/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 13/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 14/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 15/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 16/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 17/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 18/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Processing batch 19/19...\n",
      "Beginning to format 50 customers...\n",
      "Formatting customer 1/50...\n",
      "Completed formatting 50 customers\n",
      "Customer data formatted, processing 950 records...\n",
      "Calculating account age...\n",
      "Account age calculation completed\n",
      "Table customers exists.\n",
      "BigQuery table customers exists. Fetching existing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrekocev/Projects/scandiweb/magento_to_bq/venv/lib/python3.13/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning BigQuery operations...\n",
      "Data type of Customer_ID in new data: int64\n",
      "Data type of Customer_ID in existing data: object\n",
      "After conversion - Data type of Customer_ID in new data: object\n",
      "After conversion - Data type of Customer_ID in existing data: object\n",
      "Data comparison complete. Found 313 new and 0 updated records.\n",
      "Found 313 new customers to upload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1791.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New records uploaded successfully to table customers!\n",
      "No customers updates found.\n",
      "Completed processing customers data.\n",
      "Processing orders data...\n",
      "Fetching orders for 2025-01-02 (page 1)...\n",
      "Fetching orders for 2025-01-02 00:58:15 (page 2)...\n",
      "Fetching orders for 2025-01-02 02:45:15 (page 3)...\n",
      "Fetching orders for 2025-01-02 05:19:23 (page 4)...\n",
      "Fetching orders for 2025-01-02 14:26:00 (page 5)...\n",
      "Fetching orders for 2025-01-02 19:26:13 (page 6)...\n",
      "Fetching orders for 2025-01-03 00:14:58 (page 7)...\n",
      "Fetching orders for 2025-01-03 05:20:28 (page 8)...\n",
      "Fetching orders for 2025-01-03 18:34:38 (page 9)...\n",
      "No more orders found.\n",
      "Table orders exists.\n",
      "BigQuery table orders exists. Fetching existing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrekocev/Projects/scandiweb/magento_to_bq/venv/lib/python3.13/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning BigQuery operations...\n",
      "Data type of Order_ID in new data: int64\n",
      "Data type of Order_ID in existing data: object\n",
      "After conversion - Data type of Order_ID in new data: object\n",
      "After conversion - Data type of Order_ID in existing data: object\n",
      "Data comparison complete. Found 1542 new and 0 updated records.\n",
      "Found 1542 new orders to upload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New records uploaded successfully to table orders!\n",
      "No orders updates found.\n",
      "Completed processing orders data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# -------             RUN               -----\n",
    "# -------------------------------------------\n",
    "\n",
    "# Set the Google Cloud credentials environment variable\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = BQ_PATH_KEY\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client(project=BQ_PROJECT_ID)\n",
    "\n",
    "# Process customer data\n",
    "process_data_type('customers', FROM_DATE, TO_DATE, BQ_CUSTOMER_TABLE_ID, \"Customer_ID\")\n",
    "# Process order data\n",
    "process_data_type('orders', FROM_DATE, TO_DATE, BQ_ORDER_TABLE_ID, \"Order_ID\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
